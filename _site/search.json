[
  {
    "objectID": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html",
    "href": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html",
    "title": "Take-home_Ex1",
    "section": "",
    "text": "The success of Singapore’s public transport is commonly recognised as 1 of the quintessential indicator of its rapid growth and prosperity since her independence in 1965. It comprises of both public buses as well as mass rapid transport (MRT) trains. While the MRT network has been expanding across the many zones within Singapore, the bus network remains the most option accessible and reliable option amongst the two.\nThus, the utilisation pattern of buses are of key importance to multiple aspects of Singapore’s socio-economic spheres, ranging from areas of work productivity and efficiency, environmental impact, tourism, and even to potential impact to cost of living and real estate development; which underpins the impetus of this exercise to gain a better understanding of the utilisation patterns and characteristics of Singapore’s bus stops."
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#introduction",
    "href": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#introduction",
    "title": "Take-home_Ex1",
    "section": "",
    "text": "The success of Singapore’s public transport is commonly recognised as 1 of the quintessential indicator of its rapid growth and prosperity since her independence in 1965. It comprises of both public buses as well as mass rapid transport (MRT) trains. While the MRT network has been expanding across the many zones within Singapore, the bus network remains the most option accessible and reliable option amongst the two.\nThus, the utilisation pattern of buses are of key importance to multiple aspects of Singapore’s socio-economic spheres, ranging from areas of work productivity and efficiency, environmental impact, tourism, and even to potential impact to cost of living and real estate development; which underpins the impetus of this exercise to gain a better understanding of the utilisation patterns and characteristics of Singapore’s bus stops."
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#objectives",
    "href": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#objectives",
    "title": "Take-home_Ex1",
    "section": "Objectives",
    "text": "Objectives\nThe key objectives of this exercise are:\n\nTo perform geovisualisation and analysis of peakhour busstop passenger trips\nTo perform Analysis of Local Indicators of Spatial Association (LISA) for passenger trips"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#getting-started",
    "href": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#getting-started",
    "title": "Take-home_Ex1",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below load the following packages:\n\ntmap: for thematic mapping\nsf for geospatial data handling\ntidyverse for non-spatial data handling.\nknitr for creating html table\nspdep\nsfdep for creating an sf and tidyverse friendly interfaceas well as for introducing new functionality that is not present in spdep\nplotly for interactive plots\n\n\npacman::p_load(tmap, sf, tidyverse, knitr, spdep, sfdep, plotly, zoo, Kendall)"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#importing-and-preparing-the-base-geospatial-and-aspatial-data",
    "href": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#importing-and-preparing-the-base-geospatial-and-aspatial-data",
    "title": "Take-home_Ex1",
    "section": "Importing and preparing the Base Geospatial and Aspatial Data",
    "text": "Importing and preparing the Base Geospatial and Aspatial Data\n\nAspatial data\nFirstly, the Passenger Volume by Origin Destination Bus Stops data set for the month of October 2023, from LTA DataMall, will be imported by using ‘read_csv()’ of **readr* package.\n\n#\\| eval: false \nodbus &lt;- read.csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\nFrom below, the values in OROGIN_PT_CODE and DESTINATON_PT_CODE odbus in the tibble data frame are in numeric data type.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;int&gt; 4168, 4168, 80119, 80119, 44069, 20281, 20281, 190…\n$ DESTINATION_PT_CODE &lt;int&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nUsing appropriate tidyverse functions to convert these data values into factor data type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\nFrom below, the values in OROGIN_PT_CODE and DESTINATON_PT_CODE odbus in the tibble data frame are now in factor data type.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 4168, 4168, 80119, 80119, 44069, 20281, 20281, 190…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\n\n\nGeospatial data\nImport BusStop layer downloaded from LTA DataMall into RStudio and save it as a sf data frame called busstop.\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\jayexx\\ISSS624\\Take_home_Exercises\\Take_home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nThe structure of busstop sf tibble data frame is as follows.\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#objective-1-geovisualisation-and-analysis-of-bus-stop-passengers-during-peakhours",
    "href": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#objective-1-geovisualisation-and-analysis-of-bus-stop-passengers-during-peakhours",
    "title": "Take-home_Ex1",
    "section": "Objective (1): Geovisualisation and Analysis of Bus Stop Passengers during Peakhours",
    "text": "Objective (1): Geovisualisation and Analysis of Bus Stop Passengers during Peakhours\n\nExtracting the passenger trips for peakhour time intervals\nFor the purpose of this exercise, extract commuting flows during:\n\nweekday morning peak and label the output tibble data table as odbus_wkd6_9.\n\n\nodbus_wkd6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekday evening peak and label the output tibble data table as odbus_wkd17_20.\n\n\nodbus_wkd17_20 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekend/holiday morning peak and label the output tibble data table as odbus_wke11_14.\n\n\nodbus_wke11_14 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekend/holiday evening peak and label the output tibble data table as odbus_wke16_19.\n\n\nodbus_wke16_19 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nConvert and save the tible data sets in rds format as follows.\n\nwrite_rds(odbus_wkd6_9, \"data/rds/odbus_wkd6_9.rds\")\nwrite_rds(odbus_wkd17_20, \"data/rds/odbus_wkd17_20.rds\")\nwrite_rds(odbus_wke11_14, \"data/rds/odbus_wke11_14.rds\")\nwrite_rds(odbus_wke16_19, \"data/rds/odbus_wke16_19.rds\")\n\nImport files back into R as follows.\n\nodbus_wkd6_9 &lt;- read_rds(\"data/rds/odbus_wkd6_9.rds\")\nodbus_wkd17_20 &lt;- read_rds(\"data/rds/odbus_wkd17_20.rds\")\nodbus_wke11_14 &lt;- read_rds(\"data/rds/odbus_wke11_14.rds\")\nodbus_wke16_19 &lt;- read_rds(\"data/rds/odbus_wke16_19.rds\")\n\n\n\nPreparing Geospatial data in Hexagonal grid\nCreating hexagonal grid in sf format with grid_id as follows.\n\nhxgn_grid = st_make_grid(busstop, c(250, 250), what = \"polygons\", square = FALSE)\n\n# To sf and add grid ID\nhxgn_grid_sf = st_sf(hxgn_grid) %&gt;%\n  mutate(grid_id = 1:length(lengths(hxgn_grid)))\n\nMerging imported busstop data with Hexagonal grid as follows.\n\nbusstop_hxgn_grid &lt;- st_intersection(busstop, hxgn_grid_sf) %&gt;%\n  select(BUS_STOP_N, grid_id) %&gt;%\n  st_drop_geometry()\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nChecking for BUS_STOP_N with duplicate grid_id, and grid_id with duplicate BUS_STOP_N.\n\ncheck_duplicate &lt;- busstop_hxgn_grid %&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(num_BUS_STOP_N = n_distinct(BUS_STOP_N))\n\ncheck_duplicate1 &lt;- busstop_hxgn_grid %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  summarise(num_grid_id = n_distinct(grid_id))\n\nFrom the above, - out of 3,131 grid_id, 1,476 (47%) grid_id contain 2 busstops, 1,410 (45%) grid_id contain only 1 busstop, and the remaining 8% containing more than 2 busstops. Only 1 grid_id has the maximum of 5 busstops. - out of 5,145 busstops, only 8 intercept across 2 grid_id.\nHence, for this exercise, to avoid double counting of trips subsequently, the duplicate grid_id for the 8 busstops are removed.\n\nbusstop_hxgn_grid &lt;- distinct(busstop_hxgn_grid, BUS_STOP_N, .keep_all = TRUE)\n\n\n\nMerging Peakhour Passenger Trips with Geospatial data\n\nWeekday morning peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wkd6_9 data frame as follows.\n\nwkd6_9_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wkd6_9,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  ) \n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate &lt;- wkd6_9_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwkd6_9_hxgn_grid &lt;- unique(wkd6_9_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwkd6_9_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wkd6_9_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwkd6_9_hxgn_grid_sf = filter(wkd6_9_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekday evening peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wkd17_20 data frame as follows.\n\nwkd17_20_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wkd17_20,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate1 &lt;- wkd17_20_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwkd17_20_hxgn_grid &lt;- unique(wkd17_20_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwkd17_20_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wkd17_20_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwkd17_20_hxgn_grid_sf = filter(wkd17_20_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekend/holiday morning peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wke11_14 data frame as follows.\n\nwke11_14_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wke11_14,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate2 &lt;- wke11_14_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwke11_14_hxgn_grid &lt;- unique(wke11_14_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwke11_14_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wke11_14_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwke11_14_hxgn_grid_sf = filter(wke11_14_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekend/holiday evening peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wke16_19 data frame as follows.\n\nwke16_19_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wke16_19,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate3 &lt;- wke16_19_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwke16_19_hxgn_grid &lt;- unique(wke16_19_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwke16_19_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wke16_19_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwke16_19_hxgn_grid_sf = filter(wke16_19_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\n\nGeovisualisation of Busstop Peakhour periods\nPlot the map with hexagonal grid as follows for:\n\nWeekday morning peak\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(wkd6_9_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekday Morning Peak\", \"Average Passenger Trips during Weekday Morning Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekday Morning Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nWeekday evening peak\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(wkd17_20_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekday Evening Peak\", \"Average Passenger Trips during Weekday Evening Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekday Evening Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nWeekend/holiday morning peak\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(wke11_14_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekend Morning Peak\", \"Average Passenger Trips during Weekend Morning Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekend Morning Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nWeekend/holiday evening peak\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(wke16_19_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekend Evening Peak\", \"Average Passenger Trips during Weekend Evening Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekend Evening Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nSpatial patterns revealed by the geovisualisation\nFrom the Geovisualisation of the 4 peak periods, it is noted that the number of trips during the evening is higher than morning for each day type. Further based on 9 weekend/holidays and 22 weekdays in October 2023, there is higher trips per day for weekdays as compared to weekdays.\nAmongst the bus stops, the highest number of trip origins across the 4 peak periods are from the bus interchanges; while by region, the northwestern and southwestern region of Singapore seems to have the least number of trip origins across the 4 peak periods.\nMaking a comparison between distribution by Total trips and Average trips reveals whether there isn’t a very noticeable difference, and hence the areas with higher passenger trip origins were largely attributed to popular bus stops within the hexagon, rather than the number of bus stops."
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#objective-2-analysis-of-local-indicators-of-spatial-association-lisa",
    "href": "Take_home_Exercises/Take_home_Ex1/Take-home_Ex1.html#objective-2-analysis-of-local-indicators-of-spatial-association-lisa",
    "title": "Take-home_Ex1",
    "section": "Objective (2): Analysis of Local Indicators of Spatial Association (LISA)",
    "text": "Objective (2): Analysis of Local Indicators of Spatial Association (LISA)\n\nMerging Passenger Trips with Geospatial data\nAppend the merged busstop_hxgn_grid frame onto the complete odbus data frame as follows.\n\nodbus_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TOTAL_TRIPS))],\n    TOT_TRIPS = sum(TOTAL_TRIPS),\n    AVG_TRIPS = mean(TOTAL_TRIPS)\n  ) \n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate4 &lt;- odbus_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nodbus_hxgn_grid &lt;- unique(odbus_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nodbus_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           odbus_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nodbus_hxgn_grid_sf = filter(odbus_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nDeriving adaptive distance based-weights\nderive an adaptive spatial weights by using the code chunk below.\n\nwm_ad &lt;- odbus_hxgn_grid_sf %&gt;% \n  mutate(nb = st_knn(hxgn_grid,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n! Polygon provided. Using point on surface.\n\n\n\n\nComputing local Moran’s I\nCompute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package as follows.\n\nlisa &lt;- wm_ad %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\nVisualising local Moran’s I and p-value\nFor effective comparison, the Moran I and p-value maps will be plotted next to each other as shown below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran's I of Passenger Trips\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nFrom the above, it is noted that majority of the points had p-values that were more than 0.05, which meant that there were insufficient statistical evidence that the passenger trip origins were clusters or outliers relative it its closest 6 neighbours. However, there seems to be a noticeable cluster in the northwestern and southwestern region of Singapore, where the p-values are less than 0.05\nThe majority of points also had ii values that were close to 0, which meant that they low spatial correlation in passenger trip origins.\n\n\nVisualising LISA map\nThe LISA map is plotted with 2 types of outliers (High-Low and Low-High), and two type of clusters (namely: (High-High and Low-Low), which combines local Moran’s I of geographical areas and their respective p-values. For this plot, the classification in mean is used as shown below.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nAligned with the earlier analysis, noticeable/sizeable low-low clusters can be observed in the north western and southwestern region of Singapore. Notwithstanding, there are also other low-low and high-high clusters sparsely distributed across the other regions, as well as rarer occurences of high-low and low-high outliers across Singapore."
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Exercises/Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Exercises/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Exercises/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Exercises/In-class_Ex1/MPSZ-2019.html",
    "href": "In-class_Exercises/In-class_Ex1/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Exercises/In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Exercises/In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home_Ex1",
    "section": "",
    "text": "The success of Singapore’s public transport is commonly recognised as 1 of the quintessential indicator of its rapid growth and prosperity since her independence in 1965. It comprises of both public buses as well as mass rapid transport (MRT) trains. While the MRT network has been expanding across the many zones within Singapore, the bus network remains the most option accessible and reliable option amongst the two.\nThus, the utilisation pattern of buses are of key importance to multiple aspects of Singapore’s socio-economic spheres, ranging from areas of work productivity and efficiency, environmental impact, tourism, and even to potential impact to cost of living and real estate development; which underpins the impetus of this exercise to gain a better understanding of the utilisation patterns and characteristics of Singapore’s bus stops."
  },
  {
    "objectID": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#introduction",
    "href": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#introduction",
    "title": "Take-home_Ex1",
    "section": "",
    "text": "The success of Singapore’s public transport is commonly recognised as 1 of the quintessential indicator of its rapid growth and prosperity since her independence in 1965. It comprises of both public buses as well as mass rapid transport (MRT) trains. While the MRT network has been expanding across the many zones within Singapore, the bus network remains the most option accessible and reliable option amongst the two.\nThus, the utilisation pattern of buses are of key importance to multiple aspects of Singapore’s socio-economic spheres, ranging from areas of work productivity and efficiency, environmental impact, tourism, and even to potential impact to cost of living and real estate development; which underpins the impetus of this exercise to gain a better understanding of the utilisation patterns and characteristics of Singapore’s bus stops."
  },
  {
    "objectID": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#objectives",
    "href": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#objectives",
    "title": "Take-home_Ex1",
    "section": "Objectives",
    "text": "Objectives\nThe key objectives of this exercise are:\n\nTo perform geovisualisation and analysis of peakhour busstop passenger trips\nTo perform Analysis of Local Indicators of Spatial Association (LISA) for passenger trips"
  },
  {
    "objectID": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "href": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "title": "Take-home_Ex1",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below load the following packages:\n\ntmap: for thematic mapping\nsf for geospatial data handling\ntidyverse for non-spatial data handling.\nknitr for creating html table\nspdep\nsfdep for creating an sf and tidyverse friendly interfaceas well as for introducing new functionality that is not present in spdep\nplotly for interactive plots\n\n\npacman::p_load(tmap, sf, tidyverse, knitr, spdep, sfdep, plotly, zoo, Kendall)"
  },
  {
    "objectID": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#importing-and-preparing-the-base-geospatial-and-aspatial-data",
    "href": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#importing-and-preparing-the-base-geospatial-and-aspatial-data",
    "title": "Take-home_Ex1",
    "section": "Importing and preparing the Base Geospatial and Aspatial Data",
    "text": "Importing and preparing the Base Geospatial and Aspatial Data\n\nAspatial data\nFirstly, the Passenger Volume by Origin Destination Bus Stops data set for the month of October 2023, from LTA DataMall, will be imported by using ‘read_csv()’ of **readr* package.\n\n#\\| eval: false \nodbus &lt;- read.csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\nFrom below, the values in OROGIN_PT_CODE and DESTINATON_PT_CODE odbus in the tibble data frame are in numeric data type.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;int&gt; 4168, 4168, 80119, 80119, 44069, 20281, 20281, 190…\n$ DESTINATION_PT_CODE &lt;int&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nUsing appropriate tidyverse functions to convert these data values into factor data type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\nFrom below, the values in OROGIN_PT_CODE and DESTINATON_PT_CODE odbus in the tibble data frame are now in factor data type.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 4168, 4168, 80119, 80119, 44069, 20281, 20281, 190…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\n\n\nGeospatial data\nImport BusStop layer downloaded from LTA DataMall into RStudio and save it as a sf data frame called busstop.\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\jayexx\\ISSS624\\Take-home_Exercises\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nThe structure of busstop sf tibble data frame is as follows.\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…"
  },
  {
    "objectID": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#objective-1-geovisualisation-and-analysis-of-bus-stop-passengers-during-peakhours",
    "href": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#objective-1-geovisualisation-and-analysis-of-bus-stop-passengers-during-peakhours",
    "title": "Take-home_Ex1",
    "section": "Objective (1): Geovisualisation and Analysis of Bus Stop Passengers during Peakhours",
    "text": "Objective (1): Geovisualisation and Analysis of Bus Stop Passengers during Peakhours\n\nExtracting the passenger trips for peakhour time intervals\nFor the purpose of this exercise, extract commuting flows during:\n\nweekday morning peak and label the output tibble data table as odbus_wkd6_9.\n\n\nodbus_wkd6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekday evening peak and label the output tibble data table as odbus_wkd17_20.\n\n\nodbus_wkd17_20 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekend/holiday morning peak and label the output tibble data table as odbus_wke11_14.\n\n\nodbus_wke11_14 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekend/holiday evening peak and label the output tibble data table as odbus_wke16_19.\n\n\nodbus_wke16_19 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nConvert and save the tible data sets in rds format as follows.\n\nwrite_rds(odbus_wkd6_9, \"data/rds/odbus_wkd6_9.rds\")\nwrite_rds(odbus_wkd17_20, \"data/rds/odbus_wkd17_20.rds\")\nwrite_rds(odbus_wke11_14, \"data/rds/odbus_wke11_14.rds\")\nwrite_rds(odbus_wke16_19, \"data/rds/odbus_wke16_19.rds\")\n\nImport files back into R as follows.\n\nodbus_wkd6_9 &lt;- read_rds(\"data/rds/odbus_wkd6_9.rds\")\nodbus_wkd17_20 &lt;- read_rds(\"data/rds/odbus_wkd17_20.rds\")\nodbus_wke11_14 &lt;- read_rds(\"data/rds/odbus_wke11_14.rds\")\nodbus_wke16_19 &lt;- read_rds(\"data/rds/odbus_wke16_19.rds\")\n\n\n\nPreparing Geospatial data in Hexagonal grid\nCreating hexagonal grid in sf format with grid_id as follows.\n\nhxgn_grid = st_make_grid(busstop, c(250, 250), what = \"polygons\", square = FALSE)\n\n# To sf and add grid ID\nhxgn_grid_sf = st_sf(hxgn_grid) %&gt;%\n  mutate(grid_id = 1:length(lengths(hxgn_grid)))\n\nMerging imported busstop data with Hexagonal grid as follows.\n\nbusstop_hxgn_grid &lt;- st_intersection(busstop, hxgn_grid_sf) %&gt;%\n  select(BUS_STOP_N, grid_id) %&gt;%\n  st_drop_geometry()\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nChecking for BUS_STOP_N with duplicate grid_id, and grid_id with duplicate BUS_STOP_N.\n\ncheck_duplicate &lt;- busstop_hxgn_grid %&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(num_BUS_STOP_N = n_distinct(BUS_STOP_N))\n\ncheck_duplicate1 &lt;- busstop_hxgn_grid %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  summarise(num_grid_id = n_distinct(grid_id))\n\nFrom the above, - out of 3,131 grid_id, 1,476 (47%) grid_id contain 2 busstops, 1,410 (45%) grid_id contain only 1 busstop, and the remaining 8% containing more than 2 busstops. Only 1 grid_id has the maximum of 5 busstops. - out of 5,145 busstops, only 8 intercept across 2 grid_id.\nHence, for this exercise, to avoid double counting of trips subsequently, the duplicate grid_id for the 8 busstops are removed.\n\nbusstop_hxgn_grid &lt;- distinct(busstop_hxgn_grid, BUS_STOP_N, .keep_all = TRUE)\n\n\n\nMerging Peakhour Passenger Trips with Geospatial data\n\nWeekday morning peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wkd6_9 data frame as follows.\n\nwkd6_9_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wkd6_9,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  ) \n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate &lt;- wkd6_9_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwkd6_9_hxgn_grid &lt;- unique(wkd6_9_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwkd6_9_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wkd6_9_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwkd6_9_hxgn_grid_sf = filter(wkd6_9_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekday evening peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wkd17_20 data frame as follows.\n\nwkd17_20_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wkd17_20,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate1 &lt;- wkd17_20_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwkd17_20_hxgn_grid &lt;- unique(wkd17_20_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwkd17_20_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wkd17_20_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwkd17_20_hxgn_grid_sf = filter(wkd17_20_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekend/holiday morning peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wke11_14 data frame as follows.\n\nwke11_14_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wke11_14,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate2 &lt;- wke11_14_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwke11_14_hxgn_grid &lt;- unique(wke11_14_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwke11_14_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wke11_14_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwke11_14_hxgn_grid_sf = filter(wke11_14_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekend/holiday evening peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wke16_19 data frame as follows.\n\nwke16_19_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wke16_19,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate3 &lt;- wke16_19_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwke16_19_hxgn_grid &lt;- unique(wke16_19_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwke16_19_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wke16_19_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwke16_19_hxgn_grid_sf = filter(wke16_19_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\n\nGeovisualisation of Busstop Peakhour periods\nPlot the map with hexagonal grid as follows for:\n\nWeekday morning peak\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(wkd6_9_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekday Morning Peak\", \"Average Passenger Trips during Weekday Morning Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekday Morning Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekday evening peak\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(wkd17_20_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekday Evening Peak\", \"Average Passenger Trips during Weekday Evening Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekday Evening Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend/holiday morning peak\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(wke11_14_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekend Morning Peak\", \"Average Passenger Trips during Weekend Morning Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekend Morning Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend/holiday evening peak\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(wke16_19_hxgn_grid_sf) +\n  tm_fill(\n    col = c(\"TOT_TRIPS\", \"AVG_TRIPS\"),\n    palette = \"Reds\",\n    style = \"quantile\",\n    title = c(\"Total Passenger Trips during Weekend Evening Peak\", \"Average Passenger Trips during Weekend Evening Peak\"),\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Total Trips: \" = \"TOT_TRIPS\",\n      \"Average Trips: \" = \"AVG_TRIPS\",\n      \"Most Popular Bus Stop: \" = \"MAX_TRIPS_BS\"\n      \n    ),\n    popup.format = list(\n      TOT_TRIPS = list(format = \"f\", digits = 0),\n      AVG_TRIPS = list(format = \"f\", digits = 0),\n      MAX_TRIPS_BS = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_layout(main.title = \"Bus Stop Passenger Trips during Weekend Evening Peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial patterns revealed by the geovisualisation\nFrom the Geovisualisation of the 4 peak periods, it is noted that the number of trips during the evening is higher than morning for each day type. Further based on 9 weekend/holidays and 22 weekdays in October 2023, there is higher trips per day for weekdays as compared to weekdays.\nAmongst the bus stops, the highest number of trip origins across the 4 peak periods are from the bus interchanges; while by region, the northwestern and southwestern region of Singapore seems to have the least number of trip origins across the 4 peak periods.\nMaking a comparison between distribution by Total trips and Average trips reveals whether there isn’t a very noticeable difference, and hence the areas with higher passenger trip origins were largely attributed to popular bus stops within the hexagon, rather than the number of bus stops."
  },
  {
    "objectID": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#objective-2-analysis-of-local-indicators-of-spatial-association-lisa",
    "href": "Take-home_Exercises/Take-home_Ex1/Take-home_Ex1.html#objective-2-analysis-of-local-indicators-of-spatial-association-lisa",
    "title": "Take-home_Ex1",
    "section": "Objective (2): Analysis of Local Indicators of Spatial Association (LISA)",
    "text": "Objective (2): Analysis of Local Indicators of Spatial Association (LISA)\n\nMerging Passenger Trips with Geospatial data\nAppend the merged busstop_hxgn_grid frame onto the complete odbus data frame as follows.\n\nodbus_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TOTAL_TRIPS))],\n    TOT_TRIPS = sum(TOTAL_TRIPS),\n    AVG_TRIPS = mean(TOTAL_TRIPS)\n  ) \n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate4 &lt;- odbus_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nodbus_hxgn_grid &lt;- unique(odbus_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nodbus_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           odbus_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nodbus_hxgn_grid_sf = filter(odbus_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nDeriving adaptive distance based-weights\nderive an adaptive spatial weights by using the code chunk below.\n\nwm_ad &lt;- odbus_hxgn_grid_sf %&gt;% \n  mutate(nb = st_knn(hxgn_grid,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n! Polygon provided. Using point on surface.\n\n\n\n\nComputing local Moran’s I\nCompute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package as follows.\n\nlisa &lt;- wm_ad %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\nVisualising local Moran’s I and p-value\nFor effective comparison, the Moran I and p-value maps will be plotted next to each other as shown below.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran's I of Passenger Trips\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above, it is noted that majority of the points had p-values that were more than 0.05, which meant that there were insufficient statistical evidence that the passenger trip origins were clusters or outliers relative it its closest 6 neighbours. However, there seems to be a noticeable cluster in the northwestern and southwestern region of Singapore, where the p-values are less than 0.05\nThe majority of points also had ii values that were close to 0, which meant that they low spatial correlation in passenger trip origins.\n\n\nVisualising LISA map\nThe LISA map is plotted with 2 types of outliers (High-Low and Low-High), and two type of clusters (namely: (High-High and Low-Low), which combines local Moran’s I of geographical areas and their respective p-values. For this plot, the classification in mean is used as shown below.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\nAligned with the earlier analysis, noticeable/sizeable low-low clusters can be observed in the north western and southwestern region of Singapore. Notwithstanding, there are also other low-low and high-high clusters sparsely distributed across the other regions, as well as rarer occurences of high-high and low-low outliers across Singapore."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "Welcome to ISSS624 Geospatial Analytics Applications!\nIn this webpage, I am going to share with you my learning journey of geospatial analytics.\nThis is a Quarto website\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html",
    "href": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html",
    "title": "Hands-on_Ex4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/data/geospatial/Business.html",
    "href": "Take_home_Exercises/Take_home_Ex2/data/geospatial/Business.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/data/geospatial/entertn.html",
    "href": "Take_home_Exercises/Take_home_Ex2/data/geospatial/entertn.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/data/geospatial/F&B.html",
    "href": "Take_home_Exercises/Take_home_Ex2/data/geospatial/F&B.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/data/geospatial/FinServ.html",
    "href": "Take_home_Exercises/Take_home_Ex2/data/geospatial/FinServ.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/data/geospatial/Liesure&Recreation.html",
    "href": "Take_home_Exercises/Take_home_Ex2/data/geospatial/Liesure&Recreation.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/data/geospatial/MPSZ-2019.html",
    "href": "Take_home_Exercises/Take_home_Ex2/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/data/geospatial/Retails.html",
    "href": "Take_home_Exercises/Take_home_Ex2/data/geospatial/Retails.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html",
    "href": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html",
    "title": "Take-home_Ex2",
    "section": "",
    "text": "The success of Singapore’s public transport is commonly recognised as 1 of the quintessential indicator of its rapid growth and prosperity since her independence in 1965. It comprises of both public buses as well as mass rapid transport (MRT) trains. While the MRT network has been expanding across the many zones within Singapore, the bus network remains the most option accessible and reliable option amongst the two.\nThus, the utilisation pattern of buses are of key importance to multiple aspects of Singapore’s socio-economic spheres, ranging from areas of work productivity and efficiency, environmental impact, tourism, and even to potential impact to cost of living and real estate development; which underpins the impetus of this exercise to gain a better understanding of the utilisation patterns and characteristics of Singapore’s bus stops."
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#introduction",
    "href": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#introduction",
    "title": "Take-home_Ex2",
    "section": "",
    "text": "The success of Singapore’s public transport is commonly recognised as 1 of the quintessential indicator of its rapid growth and prosperity since her independence in 1965. It comprises of both public buses as well as mass rapid transport (MRT) trains. While the MRT network has been expanding across the many zones within Singapore, the bus network remains the most option accessible and reliable option amongst the two.\nThus, the utilisation pattern of buses are of key importance to multiple aspects of Singapore’s socio-economic spheres, ranging from areas of work productivity and efficiency, environmental impact, tourism, and even to potential impact to cost of living and real estate development; which underpins the impetus of this exercise to gain a better understanding of the utilisation patterns and characteristics of Singapore’s bus stops."
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#objectives",
    "href": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#objectives",
    "title": "Take-home_Ex2",
    "section": "Objectives",
    "text": "Objectives\nThe key objectives of this exercise are:\n\nTo perform geovisualisation and analysis of peakhour busstop passenger trips\nTo perform Analysis of Local Indicators of Spatial Association (LISA) for passenger trips"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#getting-started",
    "href": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#getting-started",
    "title": "Take-home_Ex2",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below load the following packages:\n\ntmap: for thematic mapping\nsf for geospatial data handling\ntidyverse for non-spatial data handling.\nknitr for creating html table\nspdep\nsfdep for creating an sf and tidyverse friendly interfaceas well as for introducing new functionality that is not present in spdep\nplotly for interactive plots\n\n\npacman::p_load(tmap, sf, tidyverse, knitr, spdep, sfdep, plotly, zoo, Kendall)"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#importing-and-preparing-the-base-geospatial-and-aspatial-data",
    "href": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#importing-and-preparing-the-base-geospatial-and-aspatial-data",
    "title": "Take-home_Ex2",
    "section": "Importing and preparing the Base Geospatial and Aspatial Data",
    "text": "Importing and preparing the Base Geospatial and Aspatial Data\n\nAspatial data\nFirstly, the Passenger Volume by Origin Destination Bus Stops data set for the month of October 2023, from LTA DataMall, will be imported by using ‘read_csv()’ of **readr* package.\n\n#\\| eval: false \nodbus &lt;- read.csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\nFrom below, the values in OROGIN_PT_CODE and DESTINATON_PT_CODE odbus in the tibble data frame are in numeric data type.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;int&gt; 4168, 4168, 80119, 80119, 44069, 20281, 20281, 190…\n$ DESTINATION_PT_CODE &lt;int&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nUsing appropriate tidyverse functions to convert these data values into factor data type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\nFrom below, the values in OROGIN_PT_CODE and DESTINATON_PT_CODE odbus in the tibble data frame are now in factor data type.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 4168, 4168, 80119, 80119, 44069, 20281, 20281, 190…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\n\n\nGeospatial data\nImport BusStop layer downloaded from LTA DataMall into RStudio and save it as a sf data frame called busstop.\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\jayexx\\ISSS624\\Take_home_Exercises\\Take_home_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nThe structure of busstop sf tibble data frame is as follows.\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…"
  },
  {
    "objectID": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#objective-1-geovisualisation-and-analysis-of-bus-stop-passengers-during-peakhours",
    "href": "Take_home_Exercises/Take_home_Ex2/Take-home_Ex2.html#objective-1-geovisualisation-and-analysis-of-bus-stop-passengers-during-peakhours",
    "title": "Take-home_Ex2",
    "section": "Objective (1): Geovisualisation and Analysis of Bus Stop Passengers during Peakhours",
    "text": "Objective (1): Geovisualisation and Analysis of Bus Stop Passengers during Peakhours\n\nExtracting the passenger trips for peakhour time intervals\nFor the purpose of this exercise, extract commuting flows during:\n\nweekday morning peak and label the output tibble data table as odbus_wkd6_9.\n\n\nodbus_wkd6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekday evening peak and label the output tibble data table as odbus_wkd17_20.\n\n\nodbus_wkd17_20 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekend/holiday morning peak and label the output tibble data table as odbus_wke11_14.\n\n\nodbus_wke11_14 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nweekend/holiday evening peak and label the output tibble data table as odbus_wke16_19.\n\n\nodbus_wke16_19 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nConvert and save the tible data sets in rds format as follows.\n\nwrite_rds(odbus_wkd6_9, \"data/rds/odbus_wkd6_9.rds\")\nwrite_rds(odbus_wkd17_20, \"data/rds/odbus_wkd17_20.rds\")\nwrite_rds(odbus_wke11_14, \"data/rds/odbus_wke11_14.rds\")\nwrite_rds(odbus_wke16_19, \"data/rds/odbus_wke16_19.rds\")\n\nImport files back into R as follows.\n\nodbus_wkd6_9 &lt;- read_rds(\"data/rds/odbus_wkd6_9.rds\")\nodbus_wkd17_20 &lt;- read_rds(\"data/rds/odbus_wkd17_20.rds\")\nodbus_wke11_14 &lt;- read_rds(\"data/rds/odbus_wke11_14.rds\")\nodbus_wke16_19 &lt;- read_rds(\"data/rds/odbus_wke16_19.rds\")\n\n\n\nPreparing Geospatial data in Hexagonal grid\nCreating hexagonal grid in sf format with grid_id as follows.\n\nhxgn_grid = st_make_grid(busstop, c(250, 250), what = \"polygons\", square = FALSE)\n\n# To sf and add grid ID\nhxgn_grid_sf = st_sf(hxgn_grid) %&gt;%\n  mutate(grid_id = 1:length(lengths(hxgn_grid)))\n\nMerging imported busstop data with Hexagonal grid as follows.\n\nbusstop_hxgn_grid &lt;- st_intersection(busstop, hxgn_grid_sf) %&gt;%\n  select(BUS_STOP_N, grid_id) %&gt;%\n  st_drop_geometry()\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nChecking for BUS_STOP_N with duplicate grid_id, and grid_id with duplicate BUS_STOP_N.\n\ncheck_duplicate &lt;- busstop_hxgn_grid %&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(num_BUS_STOP_N = n_distinct(BUS_STOP_N))\n\ncheck_duplicate1 &lt;- busstop_hxgn_grid %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  summarise(num_grid_id = n_distinct(grid_id))\n\nFrom the above, - out of 3,131 grid_id, 1,476 (47%) grid_id contain 2 busstops, 1,410 (45%) grid_id contain only 1 busstop, and the remaining 8% containing more than 2 busstops. Only 1 grid_id has the maximum of 5 busstops. - out of 5,145 busstops, only 8 intercept across 2 grid_id.\nHence, for this exercise, to avoid double counting of trips subsequently, the duplicate grid_id for the 8 busstops are removed.\n\nbusstop_hxgn_grid &lt;- distinct(busstop_hxgn_grid, BUS_STOP_N, .keep_all = TRUE)\n\n\n\nMerging Peakhour Passenger Trips with Geospatial data\n\nWeekday morning peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wkd6_9 data frame as follows.\n\nwkd6_9_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wkd6_9,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  ) \n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate &lt;- wkd6_9_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwkd6_9_hxgn_grid &lt;- unique(wkd6_9_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwkd6_9_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wkd6_9_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwkd6_9_hxgn_grid_sf = filter(wkd6_9_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekday evening peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wkd17_20 data frame as follows.\n\nwkd17_20_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wkd17_20,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate1 &lt;- wkd17_20_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwkd17_20_hxgn_grid &lt;- unique(wkd17_20_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwkd17_20_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wkd17_20_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwkd17_20_hxgn_grid_sf = filter(wkd17_20_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekend/holiday morning peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wke11_14 data frame as follows.\n\nwke11_14_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wke11_14,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate2 &lt;- wke11_14_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwke11_14_hxgn_grid &lt;- unique(wke11_14_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwke11_14_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wke11_14_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwke11_14_hxgn_grid_sf = filter(wke11_14_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\nWeekend/holiday evening peak\nAppend the merged busstop_hxgn_grid frame onto odbus_wke16_19 data frame as follows.\n\nwke16_19_hxgn_grid &lt;- left_join(busstop_hxgn_grid, odbus_wke16_19,\n            by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  rename(ORIGIN_BS = BUS_STOP_N,\n         ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  reframe(\n    MAX_TRIPS_BS = ORIGIN_BS[which.max(sum(TRIPS))],\n    TOT_TRIPS = sum(TRIPS),\n    AVG_TRIPS = mean(TRIPS)\n  )\n\nAs a good practice, check for duplicate records and retain duplicate records as follows.\n\nduplicate3 &lt;- wke16_19_hxgn_grid %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nwke16_19_hxgn_grid &lt;- unique(wke16_19_hxgn_grid)\n\nMerge resultant data with hexgon grid and filter as follows\n\nwke16_19_hxgn_grid_sf &lt;- left_join(hxgn_grid_sf, \n                           wke16_19_hxgn_grid,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))%&gt;%\n  select(MAX_TRIPS_BS, TOT_TRIPS, AVG_TRIPS, hxgn_grid)\n\nwke16_19_hxgn_grid_sf = filter(wke16_19_hxgn_grid_sf, TOT_TRIPS &gt; 0)\n\n\n\n\nGeovisualisation of Busstop Peakhour periods\nPlot the map with hexagonal grid as follows for:\n\nWeekday morning peak"
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#overview",
    "href": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#overview",
    "title": "Hands-on_Ex4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#the-data",
    "href": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#the-data",
    "title": "Hands-on_Ex4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "The Data",
    "text": "The Data\nTwo data sets will be used in this model building exercise, they are:\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL) condo_resale_2015 in csv format (i.e. condo_resale_2015.csv)"
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#getting-started",
    "href": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#getting-started",
    "title": "Hands-on_Ex4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests olsrr\nR package for calibrating geographical weighted family of models GWmodel\nR package for multivariate data visualisation and analysis corrplot\nSpatial data handling sf\nAttribute data handling\ntidyverse, especially readr, ggplot2 and dplyr Choropleth mapping tmap\n\nThe code chunks below installs and launches these R packages into R environment.\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)\n\nInstalling package into 'C:/Users/lowji/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\npackage 'GWmodel' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\lowji\\AppData\\Local\\Temp\\Rtmpewz85i\\downloaded_packages\n\n\n\nGWmodel installed\n\n\nWarning in pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, : Failed to install/load:\nGWmodel"
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#a-short-note-about-gwmodel",
    "href": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#a-short-note-about-gwmodel",
    "title": "Hands-on_Ex4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "A short note about GWmodel",
    "text": "A short note about GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#geospatial-data-wrangling",
    "href": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#geospatial-data-wrangling",
    "title": "Hands-on_Ex4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\n\nImporting geospatial data\nThe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jayexx\\ISSS624\\Hands-on_Exercises\\Hands-on_Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\nUpdating CRS information\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package.\nst_bbox(mpsz_svy21) #view extent"
  },
  {
    "objectID": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#aspatial-data-wrangling",
    "href": "Hands-on_Exercises/Hands-on_Ex4/Hands-on_Ex4.html#aspatial-data-wrangling",
    "title": "Hands-on_Ex4: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Aspatial Data Wrangling",
    "text": "Aspatial Data Wrangling\n\nImporting the aspatial data\nThe condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\nglimpse(condo_resale)\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\nNext, summary() of base R is used to display the summary statistics of cond_resale tibble data frame.\nsummary(condo_resale)\nConverting aspatial data frame into a sf object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\ncondo_resale.sf &lt;- st_as_sf(condo_resale, coords = c(“LONGITUDE”, “LATITUDE”), crs=4326) %&gt;% st_transform(crs=3414)\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\nhead(condo_resale.sf)\nNotice that the output is in point feature data frame.\n13.7 Exploratory Data Analysis (EDA) In the section, you will learn how to use statistical graphics functions of ggplot2 package to perform EDA.\n13.7.1 EDA using statistical graphics We can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nggplot(data=condo_resale.sf, aes(x=SELLING_PRICE)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\ncondo_resale.sf &lt;- condo_resale.sf %&gt;% mutate(LOG_SELLING_PRICE = log(SELLING_PRICE))\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\nggplot(data=condo_resale.sf, aes(x=LOG_SELLING_PRICE)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nNotice that the distribution is relatively less skewed after the transformation.\n13.7.2 Multiple Histogram Plots distribution of variables In this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= AREA_SQM)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= AGE)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_CBD)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_CHILDCARE)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_ELDERLYCARE)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_URA_GROWTH_AREA)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_HAWKER_MARKET)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_KINDERGARTEN)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_MRT)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_PARK)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_PRIMARY_SCH)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= PROX_TOP_PRIMARY_SCH)) + geom_histogram(bins=20, color=“black”, fill=“light blue”)\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT, PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,\nncol = 3, nrow = 4)\n13.7.3 Drawing Statistical Point Map Lastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\ntmap_mode(“view”)\nNext, the code chunks below is used to create an interactive point symbol map.\ntm_shape(mpsz_svy21)+ tm_polygons() + tm_shape(condo_resale.sf) +\ntm_dots(col = “SELLING_PRICE”, alpha = 0.6, style=“quantile”) + tm_view(set.zoom.limits = c(11,14))\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\ntmap_mode(“plot”)\n13.8 Hedonic Pricing Modelling in R In this section, you will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n13.8.1 Simple Linear Regression Method First, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\nsummary(condo.slr)\nCall: lm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n  *y = -258121.1 + 14719x1*\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\nggplot(data=condo_resale.sf,\naes(x=AREA_SQM, y=SELLING_PRICE)) + geom_point() + geom_smooth(method = lm)\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n13.8.2 Multiple Linear Regression Method\n13.8.2.1 Visualising the relationships of the independent variables Before building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = “AOE”, tl.pos = “td”, tl.cex = 0.5, method = “number”, type = “upper”)\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n13.8.3 Building a hedonic pricing model using multiple linear regression method The code chunk below using lm() to calibrate the multiple linear regression model.\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sf) summary(condo.mlr)\n13.8.4 Preparing Publication Quality Table: olsrr method With reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sf) ols_regress(condo.mlr1)\n13.8.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\ntbl_regression(condo.mlr1, intercept = TRUE)\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\ntbl_regression(condo.mlr1, intercept = TRUE) %&gt;% add_glance_source_note( label = list(sigma ~ “3C3”), include = c(r.squared, adj.r.squared, AIC, statistic, p.value, sigma))\nFor more customisation options, refer to Tutorial: tbl_regression\n13.8.5.1 Checking for multicolinearity\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\ncomprehensive regression output residual diagnostics measures of influence heteroskedasticity tests collinearity diagnostics model fit assessment variable contribution assessment variable selection procedures In the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\nols_vif_tol(condo.mlr1)\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n13.8.5.2 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\nols_plot_resid_fit(condo.mlr1)\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n13.8.5.3 Test for Normality Assumption Lastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\nols_plot_resid_hist(condo.mlr1)\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\nols_test_normality(condo.mlr1)\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n13.8.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\nNext, we will join the newly created data frame with condo_resale.sf object.\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, condo.mlr1$residuals) %&gt;% rename(MLR_RES = condo.mlr1.residuals)\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf) condo_resale.sp\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\ntmap_mode(“view”)\nThe code chunks below is used to create an interactive point symbol map.\ntm_shape(mpsz_svy21)+ tmap_options(check.and.fix = TRUE) + tm_polygons(alpha = 0.4) + tm_shape(condo_resale.res.sf) +\ntm_dots(col = “MLR_RES”, alpha = 0.6, style=“quantile”) + tm_view(set.zoom.limits = c(11,14))\nRemember to switch back to “plot” mode before continue.\ntmap_mode(“plot”)\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE) summary(nb)\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\nnb_lw &lt;- nb2listw(nb, style = ‘W’) summary(nb_lw)\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\nlm.morantest(condo.mlr1, nb_lw)\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.\n13.9 Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n13.9.1 Building Fixed Bandwidth GWR Model\n13.9.1.1 Computing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, approach=“CV”, kernel=“gaussian”, adaptive=FALSE, longlat=FALSE)\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n13.9.1.2 GWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, bw=bw.fixed, kernel = ‘gaussian’, longlat = FALSE)\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\ngwr.fixed\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n13.9.2 Building Adaptive Bandwidth GWR Model In this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n13.9.2.1 Computing the adaptive bandwidth Similar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, approach=“CV”, kernel=“gaussian”, adaptive=TRUE, longlat=FALSE)\nThe result shows that the 30 is the recommended data points to be used.\n13.9.2.2 Constructing the adaptive bandwidth gwr model Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, bw=bw.adaptive, kernel = ‘gaussian’, adaptive=TRUE, longlat = FALSE)\nThe code below can be used to display the model output.\ngwr.adaptive\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n13.9.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n13.9.4 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;% st_transform(crs=3414)\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414) condo_resale.sf.adaptive.svy21\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF) condo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\nglimpse(condo_resale.sf.adaptive)\nThe code chunks below is used to create an interactive point symbol map.\ntmap_mode(“view”) tm_shape(mpsz_svy21)+ tm_polygons(alpha = 0.1) + tm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = “Local_R2”, border.col = “gray60”, border.lwd = 1) + tm_view(set.zoom.limits = c(11,14))\ntmap_mode(“plot”)\n13.9.6 Visualising coefficient estimates The code chunks below is used to create an interactive point symbol map.\ntmap_mode(“view”) AREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+ tm_polygons(alpha = 0.1) + tm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = “AREA_SQM_SE”, border.col = “gray60”, border.lwd = 1) + tm_view(set.zoom.limits = c(11,14))\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+ tm_polygons(alpha = 0.1) + tm_shape(condo_resale.sf.adaptive) +\ntm_dots(col = “AREA_SQM_TV”, border.col = “gray60”, border.lwd = 1) + tm_view(set.zoom.limits = c(11,14))\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, asp=1, ncol=2, sync = TRUE)\ntmap_mode(“plot”)\n13.9.6.1 By URA Plannign Region tm_shape(mpsz_svy21[mpsz_svy21$REGION_N==“CENTRAL REGION”, ])+ tm_polygons()+ tm_shape(condo_resale.sf.adaptive) + tm_bubbles(col = “Local_R2”, size = 0.15, border.col = “gray60”, border.lwd = 1)"
  }
]