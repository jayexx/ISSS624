---
title: "Hands-on_Ex2-2"
author: "J X Low"
---

## Overview

In this hands-on exercise, I learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) in R, using **sf** (*for import*), **readr** (*for import*), **dplyr** (*for relational join*), **tmap** (*for output visualisation analysis*), and **spdep** (*for the areas below*) packages.

-   compute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,
    -   plot Moran scatterplot,
    -   compute and plot spatial correlogram using appropriate function of spdep package.
-   compute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;
-   compute Getis-Ord's Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and

## Getting Started

### The analytical question

In spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”

In this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of [Hunan Provice, People Republic of China](https://en.wikipedia.org/wiki/Hunan).

### Datasets

Data set consists of:

- Hunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.
- Hunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.

### Loading packages

The code chunk below installs and loads [spdep](https://cran.r-project.org/web/packages/spdep/spdep.pdf) [sf](https://cran.r-project.org/web/packages/sf/sf.pdf), [tmap](https://cran.r-project.org/web/packages/tmap/tmap.pdf) and tidyverse packages into R environment

```{r}
pacman:: p_load(dplyr, readr, tmap, sf, tidyverse, spdep, knitr)
```

## Importing Data

Raw data files were obtained for Hunan from e-learn.

### Import shapefile into r environment

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

### Importing Attribute Data (csv file)

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv", show_col_types = FALSE)
```

### Data Preparation (relational join)

Update the attribute table of Hunan's Spatial Polygons DataFrame with the attribute fields of hunan2012 DataFrame by using left_join() of dplyr package, with the following code chunk

```{R}
hunan <- left_join(hunan,hunan2012)%>%
  select(1:4, 7, 15)
```

### Visualising Regional Development Indicator

Prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package

```{R}
equal <- tm_shape(hunan) +
  tm_fill("GDPPC",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(hunan) +
  tm_fill("GDPPC",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)
```

## Global Spatial Autocorrelation

### Computing Contiguity Spatial Weights

1st, construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.

In the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.

More specifically, the code chunk below is used to compute Queen contiguity weight matrix.

























